---
layout: post
title: Blog Post 4 Fake News Classification
---

Rampant misinformation—often called “fake news”—is one of the defining features of contemporary democratic life. In this Blog Post, you will develop and assess a fake news classifier using Tensorflow.

**Note: Working on this Blog Post in Google Colab is highly recommended.**

<h1>§1. Acquire Training Data</h1>

We have hosted a training data set at the below URL. You can either read it into Python directly <em>(via pd.read_csv()) </em> or download it to your computer and read it from disk.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```

Firstly, we need import required packages.

```python
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import string
import re
```

```python
# Read the data set
df = pd.read_csv(train_url)
df.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>


Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.

<h1>§2. Make a Dataset</h1>

Write a function called make_dataset. This function should do two things:

• *Remove stopwords* from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.

• Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.

Call the function make_dataset on your training dataframe to produce a Dataset. You may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.

```python
# Import stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = stopwords.words('english')
```

```
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
```

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["fake"] = le.fit_transform(df["fake"])
num_title = len(df["fake"].unique())
num_title
```

```
2
```

```python
def make_dataset(df):
  
  # remove stopwords from text and title
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  
  # contruct the dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
          {
          "title" : df[["title"]],
          "text"  : df[["text"]]
           },
       {
           "fake" : df[["fake"]]
        }
       )
      )
  # increase the speed of training
  data.batch(100)
  return data
```

<h2>Validation Data</h2>

After you’ve constructed your primary Dataset, split of 20% of it to use for validation.

```python
data=make_dataset(df)
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train = data.take(train_size).batch(20)
val = data.skip(train_size).take(val_size).batch(20)

print(len(train), len(val))
```

```python
898 225
```

<h2>Base Rate</h2>

Recall that the base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Determine the base rate for this data set by examining the labels on the training set.

```python
df['fake'].value_counts()
```

```
1    11740
0    10709
Name: fake, dtype: int64
```

```python
11740/(11740+10709)
```

```
0.522963160942581
```

So, the base rate is 52.3%.

<h2>TextVectorization</h2>

```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras import Input, layers, Model, utils, losses
#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

<h1>§3. Create Models</h1>

Please use TensorFlow models to offer a perspective on the following question:

<blockquote>
  <p>When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?</p>
</blockquote>

<p>To address this question, create <strong>three (3)</strong> TensorFlow models.</p>

<ol>
  <li>In the first model, you should use <strong>only the article title</strong> as an input.</li>
  <li>In the second model, you should use <strong>only the article text</strong> as an input.</li>
  <li>In the third model, you should use <strong>both the article title and the article text</strong> as input.</li>
</ol>

Train your models on the training data until they appear to be “fully” trained. Assess and compare their performance. Make sure to include a visualization of the training histories.

<h3>Notes</h3>

<ul>
  <li>For the first two models, you don’t have to create new <code class="language-plaintext highlighter-rouge">Dataset</code>s. Instead, just specify the <code class="language-plaintext highlighter-rouge">inputs</code> to the <code class="language-plaintext highlighter-rouge">keras.Model</code> appropriately, and TensorFlow will automatically ignore the unused inputs in the <code class="language-plaintext highlighter-rouge">Dataset</code>.</li>
  <li>The lecture notes and tutorials linked above are likely to be helpful as you are creating your models as well.</li>
  <li><strong>You will need to use the Functional API, rather than the Sequential API, for this modeling task.</strong></li>
  <li>When using the Functional API, it is possible to use the same layer in multiple parts of your model; see <a href="https://www.tensorflow.org/guide/keras/functional">this tutorial</a> for examples. I recommended that you share an embedding layer for both the article <code class="language-plaintext highlighter-rouge">title</code> and <code class="language-plaintext highlighter-rouge">text</code> inputs.</li>
  <li><strong>You may encounter overfitting</strong>, in which case <code class="language-plaintext highlighter-rouge">Dropout</code> layers can help.</li>
</ul>

<h2>Title Model</h2>

• In the first model, you should use only the article title as an input.

```python
# title model input
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

# set embedding layer
embedding = layers.Embedding(size_vocabulary, 16, name = "embedding")

# pipeline
title_features = title_vectorize_layer(title_input)
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

# model output
output_1 = layers.Dense(2, name = "fake")(title_features)
```

Here is the first model:

```python
model_title = keras.Model(
    inputs = [title_input],
    outputs = output_1
)

```

```python
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)

history_title = model_title.fit(train,
                    validation_data = val,
                    epochs = 20,
                    verbose = False)

```

```python
from matplotlib import pyplot as plt
plt.plot(history_title.history["accuracy"], label = "training")
plt.plot(history_title.history["val_accuracy"], label = "validation")
plt.legend()
```

![hw4pic1.png](/images/hw4pic1.png)

The baseline accuracy is 52.3%. We can see the validation accuracy is bigger than baseline accuracy.

<h2>Text Model</h2>

• In the second model, you should use only the article text as an input.

```python
# text model input
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)

# pipeline for title
# share an embedding layer for both the article title and text inputs
text_features = title_vectorize_layer(text_input) 
text_features = embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

# model output
output_2 = layers.Dense(2, name = "fake")(text_features)

```

```python
model_text = keras.Model(
    inputs = [text_input],
    outputs = output_2
)
```

```python
model_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)

history_text = model_text.fit(train,
                    validation_data = val,
                    epochs = 20,
                    verbose = False)
```

```python
from matplotlib import pyplot as plt
plt.plot(history_text.history["accuracy"], label = "training")
plt.plot(history_text.history["val_accuracy"], label = "validation")
plt.legend()
```

![hw4pic2.png](/images/hw4pic2.png)

The validation accuracy of model can reach 97% when training.

<h2>Title & Text Model</h2>

• In the third model, you should use both the article title and the article text as input.

For the First step we need to combine title_features and text_features to create our new model. 

```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation = 'relu')(main)
# output
main_output = layers.Dense(2, name = "fake")(main)
```

```python
model_3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = main_output
)
```

```python
model_3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
history_3 = model_3.fit(train,
                    validation_data = val,
                    epochs = 20,
                    verbose = False)
```

```python
from matplotlib import pyplot as plt
plt.plot(history_3.history["accuracy"], label = "training")
plt.plot(history_3.history["val_accuracy"], label = "validation")
plt.legend()
```

![hw4pic3.png](/images/hw4pic3.png)

According the result,  we can see that the validation accuracy of model can stabilize above 97% during training. In that case we know that when we trying to detect fake news, algorithms should use both text and title to get higher accuracy.

<h1>§4. Model Evaluation</h1>

Now we’ll test your model performance on unseen test data. For this part, you can focus on your **best** model, and ignore the other two.

Once you’re satisfied with your best model’s performance on validation data, download the test data here:

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```

You’ll need to convert this data using the make_dataset function you defined in Part §2. Then, evaluate your model on the data. 

```python
test = pd.read_csv(test_url)
test_dataset = make_dataset(test)
model_3.evaluate(test_dataset)
```

```
22449/22449 [==============================] - 77s 3ms/step - loss: 0.0333 - accuracy: 0.9930
[0.03326217457652092, 0.9930063486099243]
```

The validation accuracy is **99.30%**. The result can show us that it’s a good model.

<h1>§5. Embedding Visualization</h1>

Visualize and comment on the embedding that your model learned (you did use an embedding, right?). Are you able to find any interesting patterns or associations in the words that the model found useful when distinguishing real news from fake news? You are welcome to use either 2-dimensional or 3-dimensional embedding. Comment on at least 5 words whose location in the embedding you find interpretable.


```python

# get the weights from the embedding layer
weights = model_3.get_layer('embedding').get_weights()[0]

# get the vocabulary
vocab = title_vectorize_layer.get_vocabulary()

# import PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab,
    'x0' : weights[:,0],
    'x1' : weights[:,1]
    })

```

After use the PCA, we can draw the plot:

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                # size_max = 2,
                 hover_name = "word")

fig.show()
```


{% include hw4figure1.html %}

Everything looks nice! We’ve successfully visualized our embedding. From the above visualization, we found that some points of words are far away from the center of the plot. For example, **gop, trumps, factbox, video, reportedly, rep.**



