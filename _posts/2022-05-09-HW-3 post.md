---
layout: post
title: Blog Post 3 Image Classification
---

In this blog post, you will learn several new skills and concepts related to image classification in Tensorflow.\
• Tensorflow <code class="language-plaintext highlighter-rouge">Datasets</code> provide a convenient way for us to organize operations on our training, validation, and test data sets.

• <em>Data augmentation</em> allows us to create expanded versions of our data sets that allow models to learn patterns more robustly.

• <em>Transfer learning</em> allows us to use pre-trained models for new tasks.

Working on the coding portion of the Blog Post in Google Colab is strongly recommended. When training your model, enabling a GPU runtime (under Runtime -> Change Runtime Type) is likely to lead to significant speed benefits.


<h1>§1. Load Packages and Obtain Data</h1>
First of all, we need to import some necessary packages.

```python
import os
import tensorflow as tf
from tensorflow.keras import utils
from matplotlib import pyplot as plt
import numpy as np
import random
from tensorflow.keras import datasets, layers, models
```

Now, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.

Paste and run the following code block.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

```
Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
```

Paste the following code into the next block. This is technical code related to rapidly reading data. If you’re interested in learning more about this kind of thing, you can take a look <a href="https://www.tensorflow.org/guide/data_performance">here</a>.

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

<h2>Working with Datasets</h2>

You can get a piece of a data set using the take method; e.g. train_dataset.take(1) will retrieve one batch (32 images with labels) from the training data.

Let’s briefly explore our data set. **Write a function to create a two-row visualization**. In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs.  A docstring is not required.

```python
def cat_dog():
  class_names = ['cats', 'dogs']

  for images, labels in train_dataset.take(1):
    plt.figure(figsize=(10, 10))
    cat=images[labels==0]
    dog=images[labels==1]
    for i in range(6):
      ax = plt.subplot(3, 3, i + 1)
      if i <= 2:
        plt.imshow(cat[i].numpy().astype("uint8"))
        plt.title("cat")
        plt.axis("off")
      else:
        plt.imshow(dog[i].numpy().astype("uint8"))
        plt.title("dog")
        plt.axis("off")
```

```
cat_dog()
```

![hw3pic1.png](/images/hw3pic1.png)

Looks great! The first row shows three images of cats, and the second row shows three images of dogs!

<h2>Check Label Frequencies</h2>

The following line of code will create an iterator called labels.

```python
labels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

<h1>§2. First Model</h1>

Create a tf.keras.Sequential model using some of the layers we’ve discussed in class. In each model, include at least two Conv2D layers, at least two MaxPooling2D layers, at least one Flatten layer, at least one Dense layer, and at least one Dropout layer. Train your model and plot the history of the accuracy on both the training and validation sets. Give your model the name model1.

```python
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dropout(.2),
    layers.Dense(2)
])
```

training the model:

```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

```
Epoch 1/20
63/63 [==============================] - 4s 56ms/step - loss: 3.3262 - accuracy: 0.5140 - val_loss: 0.7010 - val_accuracy: 0.5272
Epoch 2/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6768 - accuracy: 0.5590 - val_loss: 0.7018 - val_accuracy: 0.5322
Epoch 3/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6301 - accuracy: 0.6230 - val_loss: 0.7036 - val_accuracy: 0.5297
Epoch 4/20
63/63 [==============================] - 4s 55ms/step - loss: 0.5843 - accuracy: 0.6560 - val_loss: 0.7486 - val_accuracy: 0.5161
Epoch 5/20
63/63 [==============================] - 4s 54ms/step - loss: 0.5610 - accuracy: 0.6840 - val_loss: 0.7468 - val_accuracy: 0.5532
Epoch 6/20
63/63 [==============================] - 4s 55ms/step - loss: 0.5209 - accuracy: 0.7175 - val_loss: 0.8304 - val_accuracy: 0.5606
Epoch 7/20
63/63 [==============================] - 4s 55ms/step - loss: 0.4855 - accuracy: 0.7350 - val_loss: 0.9289 - val_accuracy: 0.5532
Epoch 8/20
63/63 [==============================] - 4s 53ms/step - loss: 0.4737 - accuracy: 0.7465 - val_loss: 0.8909 - val_accuracy: 0.5767
Epoch 9/20
63/63 [==============================] - 4s 52ms/step - loss: 0.4430 - accuracy: 0.7645 - val_loss: 0.9086 - val_accuracy: 0.5545
Epoch 10/20
63/63 [==============================] - 4s 53ms/step - loss: 0.4214 - accuracy: 0.7885 - val_loss: 0.9665 - val_accuracy: 0.5743
Epoch 11/20
63/63 [==============================] - 4s 53ms/step - loss: 0.4045 - accuracy: 0.8100 - val_loss: 1.1087 - val_accuracy: 0.5792
Epoch 12/20
63/63 [==============================] - 4s 54ms/step - loss: 0.3430 - accuracy: 0.8385 - val_loss: 1.0543 - val_accuracy: 0.5681
Epoch 13/20
63/63 [==============================] - 4s 53ms/step - loss: 0.3231 - accuracy: 0.8540 - val_loss: 1.1052 - val_accuracy: 0.6139
Epoch 14/20
63/63 [==============================] - 4s 54ms/step - loss: 0.3094 - accuracy: 0.8590 - val_loss: 1.1491 - val_accuracy: 0.5953
Epoch 15/20
63/63 [==============================] - 4s 53ms/step - loss: 0.2923 - accuracy: 0.8695 - val_loss: 1.2250 - val_accuracy: 0.5953
Epoch 16/20
63/63 [==============================] - 4s 54ms/step - loss: 0.2499 - accuracy: 0.8860 - val_loss: 1.3459 - val_accuracy: 0.5619
Epoch 17/20
63/63 [==============================] - 4s 55ms/step - loss: 0.2810 - accuracy: 0.8760 - val_loss: 1.3877 - val_accuracy: 0.6015
Epoch 18/20
63/63 [==============================] - 4s 53ms/step - loss: 0.2513 - accuracy: 0.9055 - val_loss: 1.4519 - val_accuracy: 0.5817
Epoch 19/20
63/63 [==============================] - 4s 53ms/step - loss: 0.2470 - accuracy: 0.9005 - val_loss: 1.6971 - val_accuracy: 0.5767
Epoch 20/20
63/63 [==============================] - 4s 53ms/step - loss: 0.2311 - accuracy: 0.8980 - val_loss: 1.5966 - val_accuracy: 0.6002
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![hw3pic2.png](/images/hw3pic2.png)

The validation accuracy of model stabilized **between 51% and 61%** during training.

The baseline accuracy is 50%. And the validation accuracy is bigger than baseline accuracy.

From the plot our training accuracy is much higher than the validation accuracy, so model 1 is overfitting.

<h1>§3. Model with Data Augmentation</h1>

First, create a tf.keras.layers.RandomFlip() layer.

```python
RandomFlip = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')
])
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = RandomFlip(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![hw3pic3.png](/images/hw3pic3.png)

```python
RandomRotation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.25)
])
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = RandomRotation(tf.expand_dims(first_image, 0),training=True)
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```

![hw3pic4.png](/images/hw3pic4.png)

```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
])
```

Here is model 2!

```python
# model 2
model2 = tf.keras.Sequential([
    data_augmentation,

    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dropout(.2),
    layers.Dense(64, activation='relu'),
    layers.Dense(2)
])
```

Then we train the model:

```python
model2.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
history_2 = model2.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

```
Epoch 1/20
63/63 [==============================] - 5s 58ms/step - loss: 8.7135 - accuracy: 0.4995 - val_loss: 0.6897 - val_accuracy: 0.5644
Epoch 2/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6882 - accuracy: 0.5620 - val_loss: 0.6783 - val_accuracy: 0.5804
Epoch 3/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6914 - accuracy: 0.5295 - val_loss: 0.6850 - val_accuracy: 0.5458
Epoch 4/20
63/63 [==============================] - 4s 57ms/step - loss: 0.6862 - accuracy: 0.5475 - val_loss: 0.6737 - val_accuracy: 0.5705
Epoch 5/20
63/63 [==============================] - 4s 57ms/step - loss: 0.6839 - accuracy: 0.5765 - val_loss: 0.6629 - val_accuracy: 0.6015
Epoch 6/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6793 - accuracy: 0.5855 - val_loss: 0.6653 - val_accuracy: 0.6101
Epoch 7/20
63/63 [==============================] - 4s 58ms/step - loss: 0.6647 - accuracy: 0.5955 - val_loss: 0.6683 - val_accuracy: 0.5891
Epoch 8/20
63/63 [==============================] - 4s 57ms/step - loss: 0.6559 - accuracy: 0.6185 - val_loss: 0.6661 - val_accuracy: 0.6027
Epoch 9/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6495 - accuracy: 0.6245 - val_loss: 0.6686 - val_accuracy: 0.5780
Epoch 10/20
63/63 [==============================] - 4s 56ms/step - loss: 0.6597 - accuracy: 0.6130 - val_loss: 0.6702 - val_accuracy: 0.5681
Epoch 11/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6565 - accuracy: 0.5955 - val_loss: 0.6548 - val_accuracy: 0.5941
Epoch 12/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6394 - accuracy: 0.6300 - val_loss: 0.6392 - val_accuracy: 0.6163
Epoch 13/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6587 - accuracy: 0.6095 - val_loss: 0.6499 - val_accuracy: 0.6040
Epoch 14/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6402 - accuracy: 0.6390 - val_loss: 0.6345 - val_accuracy: 0.6324
Epoch 15/20
63/63 [==============================] - 4s 66ms/step - loss: 0.6349 - accuracy: 0.6350 - val_loss: 0.6395 - val_accuracy: 0.6287
Epoch 16/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6405 - accuracy: 0.6350 - val_loss: 0.6615 - val_accuracy: 0.6114
Epoch 17/20
63/63 [==============================] - 4s 57ms/step - loss: 0.6556 - accuracy: 0.6065 - val_loss: 0.6575 - val_accuracy: 0.5854
Epoch 18/20
63/63 [==============================] - 4s 56ms/step - loss: 0.6809 - accuracy: 0.5465 - val_loss: 0.6733 - val_accuracy: 0.5705
Epoch 19/20
63/63 [==============================] - 4s 56ms/step - loss: 0.6477 - accuracy: 0.6395 - val_loss: 0.6286 - val_accuracy: 0.6473
Epoch 20/20
63/63 [==============================] - 4s 55ms/step - loss: 0.6265 - accuracy: 0.6550 - val_loss: 0.6262 - val_accuracy: 0.6498
```

```python
plt.plot(history_2.history["accuracy"], label = "training")
plt.plot(history_2.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![hw3pic5.png](/images/hw3pic5.png)

The validation accuracy of model stabilized **between 54.5% and 64.9%** during training.

The validation accuracy of model 2 is generally higher than that of model 1.

We can see that the overfitting problem is getting better now beacause of data agumentation.

<h1>§4. Data Preprocessing</h1>

Sometimes, it can be helpful to make simple transformations to the input data. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.

The following code will create a preprocessing layer called preprocessor which you can slot into your model pipeline.

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

```python
model3 = models.Sequential([
    preprocessor,
    data_augmentation,
    
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dropout(.2),
    layers.Dense(64, activation='relu'),
    layers.Dense(2)
])
```

Train our model 3:

```python
model3.compile(optimizer='adam', 
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
history_3 = model3.fit(train_dataset, 
           epochs=20, 
           validation_data=validation_dataset)
```

```
Epoch 1/20
63/63 [==============================] - 4s 56ms/step - loss: 0.6981 - accuracy: 0.5405 - val_loss: 0.6588 - val_accuracy: 0.6002
Epoch 2/20
63/63 [==============================] - 4s 56ms/step - loss: 0.6695 - accuracy: 0.5850 - val_loss: 0.6442 - val_accuracy: 0.6399
Epoch 3/20
63/63 [==============================] - 4s 53ms/step - loss: 0.6235 - accuracy: 0.6400 - val_loss: 0.6062 - val_accuracy: 0.6287
Epoch 4/20
63/63 [==============================] - 4s 53ms/step - loss: 0.6004 - accuracy: 0.6620 - val_loss: 0.5784 - val_accuracy: 0.6609
Epoch 5/20
63/63 [==============================] - 4s 54ms/step - loss: 0.5875 - accuracy: 0.6845 - val_loss: 0.5919 - val_accuracy: 0.6708
Epoch 6/20
63/63 [==============================] - 4s 54ms/step - loss: 0.5835 - accuracy: 0.6980 - val_loss: 0.5679 - val_accuracy: 0.7054
Epoch 7/20
63/63 [==============================] - 4s 53ms/step - loss: 0.5534 - accuracy: 0.7125 - val_loss: 0.5521 - val_accuracy: 0.6980
Epoch 8/20
63/63 [==============================] - 5s 70ms/step - loss: 0.5644 - accuracy: 0.7095 - val_loss: 0.5844 - val_accuracy: 0.6931
Epoch 9/20
63/63 [==============================] - 4s 54ms/step - loss: 0.5413 - accuracy: 0.7275 - val_loss: 0.6025 - val_accuracy: 0.6844
Epoch 10/20
63/63 [==============================] - 6s 94ms/step - loss: 0.5388 - accuracy: 0.7255 - val_loss: 0.5351 - val_accuracy: 0.7314
Epoch 11/20
63/63 [==============================] - 5s 70ms/step - loss: 0.5324 - accuracy: 0.7215 - val_loss: 0.5228 - val_accuracy: 0.7327
Epoch 12/20
63/63 [==============================] - 4s 56ms/step - loss: 0.5095 - accuracy: 0.7540 - val_loss: 0.5531 - val_accuracy: 0.7339
Epoch 13/20
63/63 [==============================] - 4s 55ms/step - loss: 0.5098 - accuracy: 0.7375 - val_loss: 0.5464 - val_accuracy: 0.7475
Epoch 14/20
63/63 [==============================] - 4s 54ms/step - loss: 0.5018 - accuracy: 0.7535 - val_loss: 0.4965 - val_accuracy: 0.7525
Epoch 15/20
63/63 [==============================] - 4s 56ms/step - loss: 0.4938 - accuracy: 0.7635 - val_loss: 0.5197 - val_accuracy: 0.7512
Epoch 16/20
63/63 [==============================] - 4s 55ms/step - loss: 0.4946 - accuracy: 0.7565 - val_loss: 0.5160 - val_accuracy: 0.7500
Epoch 17/20
63/63 [==============================] - 4s 54ms/step - loss: 0.4775 - accuracy: 0.7635 - val_loss: 0.5102 - val_accuracy: 0.7463
Epoch 18/20
63/63 [==============================] - 4s 54ms/step - loss: 0.4696 - accuracy: 0.7715 - val_loss: 0.5237 - val_accuracy: 0.7463
Epoch 19/20
63/63 [==============================] - 4s 53ms/step - loss: 0.4563 - accuracy: 0.7870 - val_loss: 0.4732 - val_accuracy: 0.7710
Epoch 20/20
63/63 [==============================] - 4s 54ms/step - loss: 0.4576 - accuracy: 0.7820 - val_loss: 0.4854 - val_accuracy: 0.7686
```

```python
plt.plot(history_3.history["accuracy"], label = "training")
plt.plot(history_3.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![hw3pic6.png](/images/hw3pic6.png)

The validation accuracy of model stabilized **between 60% and 77%** during training.

The validation accuracy of model 3 is much higher than that of model 1. This shows that model 3 can better classify.

And the overfitting problem is way more better then before.


<h1>§5. Transfer Learning</h1>

So far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Maybe we could use a pre-existing model for our task?

To do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.

Paste the following code in order to download MobileNetV2 and configure it as a layer that can be included in your model.

```
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

```
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
9412608/9406464 [==============================] - 0s 0us/step
9420800/9406464 [==============================] - 0s 0us/step
```

Create a model called model4 that uses MobileNetV2. For this, you should definitely use the following layers:

1. The preprocessor layer from Part §4.
2. The **data augmentation** layers from Part §3.
3. The base_model_layer constructed above.
4. A Dense(2) layer at the very end to actually perform the classification.

Make sure that you are able to achieve **at least 95% validation accuracy.**

here's my model4: 

```python
model4 = tf.keras.Sequential([
    preprocessor,
    data_augmentation, 
    base_model_layer,

    layers.GlobalMaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(2)
])
```

```python
model4.summary()
```

```
Model: "sequential_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 model_1 (Functional)        (None, 160, 160, 3)       0         
                                                                 
 sequential_16 (Sequential)  (None, 160, 160, 3)       0         
                                                                 
 model_2 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                 
 global_max_pooling2d (Globa  (None, 1280)             0         
 lMaxPooling2D)                                                  
                                                                 
 flatten_8 (Flatten)         (None, 1280)              0         
                                                                 
 dropout_8 (Dropout)         (None, 1280)              0         
                                                                 
 dense_12 (Dense)            (None, 2)                 2562      
                                                                 
=================================================================
Total params: 2,260,546
Trainable params: 2,562
Non-trainable params: 2,257,984
_________________________________________________________________
```

Now, training the data:

```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
history_4= model4.fit(train_dataset, 
                     epochs=20, 
```

```
Epoch 1/20
63/63 [==============================] - 8s 75ms/step - loss: 1.3754 - accuracy: 0.7610 - val_loss: 0.1249 - val_accuracy: 0.9666
Epoch 2/20
63/63 [==============================] - 4s 58ms/step - loss: 0.6463 - accuracy: 0.8830 - val_loss: 0.1032 - val_accuracy: 0.9703
Epoch 3/20
63/63 [==============================] - 4s 58ms/step - loss: 0.5776 - accuracy: 0.8925 - val_loss: 0.0743 - val_accuracy: 0.9765
Epoch 4/20
63/63 [==============================] - 4s 57ms/step - loss: 0.4555 - accuracy: 0.9125 - val_loss: 0.0641 - val_accuracy: 0.9777
Epoch 5/20
63/63 [==============================] - 4s 58ms/step - loss: 0.4463 - accuracy: 0.9100 - val_loss: 0.0719 - val_accuracy: 0.9827
Epoch 6/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4532 - accuracy: 0.9140 - val_loss: 0.0624 - val_accuracy: 0.9802
Epoch 7/20
63/63 [==============================] - 4s 58ms/step - loss: 0.4193 - accuracy: 0.9115 - val_loss: 0.0672 - val_accuracy: 0.9827
Epoch 8/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4062 - accuracy: 0.9195 - val_loss: 0.0581 - val_accuracy: 0.9864
Epoch 9/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4208 - accuracy: 0.9170 - val_loss: 0.0721 - val_accuracy: 0.9802
Epoch 10/20
63/63 [==============================] - 4s 60ms/step - loss: 0.3208 - accuracy: 0.9285 - val_loss: 0.0649 - val_accuracy: 0.9827
Epoch 11/20
63/63 [==============================] - 5s 70ms/step - loss: 0.3395 - accuracy: 0.9255 - val_loss: 0.0700 - val_accuracy: 0.9802
Epoch 12/20
63/63 [==============================] - 4s 59ms/step - loss: 0.3185 - accuracy: 0.9235 - val_loss: 0.0514 - val_accuracy: 0.9851
Epoch 13/20
63/63 [==============================] - 4s 60ms/step - loss: 0.3567 - accuracy: 0.9275 - val_loss: 0.0651 - val_accuracy: 0.9814
Epoch 14/20
63/63 [==============================] - 4s 58ms/step - loss: 0.2545 - accuracy: 0.9330 - val_loss: 0.0898 - val_accuracy: 0.9802
Epoch 15/20
63/63 [==============================] - 4s 60ms/step - loss: 0.3278 - accuracy: 0.9280 - val_loss: 0.1390 - val_accuracy: 0.9666
Epoch 16/20
63/63 [==============================] - 4s 59ms/step - loss: 0.2585 - accuracy: 0.9365 - val_loss: 0.0867 - val_accuracy: 0.9703
Epoch 17/20
63/63 [==============================] - 5s 74ms/step - loss: 0.2899 - accuracy: 0.9275 - val_loss: 0.0841 - val_accuracy: 0.9715
Epoch 18/20
63/63 [==============================] - 4s 61ms/step - loss: 0.2979 - accuracy: 0.9310 - val_loss: 0.0594 - val_accuracy: 0.9814
Epoch 19/20
63/63 [==============================] - 4s 61ms/step - loss: 0.2639 - accuracy: 0.9315 - val_loss: 0.0498 - val_accuracy: 0.9814
Epoch 20/20
63/63 [==============================] - 4s 60ms/step - loss: 0.2833 - accuracy: 0.9275 - val_loss: 0.0537 - val_accuracy: 0.9827
```

```python
plt.plot(history_4.history["accuracy"], label = "training")
plt.plot(history_4.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![hw3pic7.png](/images/hw3pic7.png)



The validation accuracy of model stabilized **between 96% and 98%** during training.

The validation accuracy of this model is much higher than model 1, model 2 and model 3.

There is no overfitting here because the training accuracy is always lower than the validation accuracy.

<h1>§6. Score on Test Data</h1>

```python
model4.evaluate(test_dataset)
```

```
6/6 [==============================] - 0s 41ms/step - loss: 0.0321 - accuracy: 0.9844
[0.03208949789404869, 0.984375]
```

From the above output, we can see that the accuracy is about **98.4%** which is awesome. Comparing to the first model we create, that's a huge improvement!